---
title: 'On the presentation of data: No more than two significant digits, please'
author: "Zach Kunicki"
date: "9/26/2020"
output: html_document
---

# On the presentation of data: No more than two significant digits, please

## **A**dvanced **QUA**n**TI**tative topics to **C**heckout (AQUATIC) Blog

### Zach Kunicki, QSP Postdoc

Hey AQUATIC readers! If you’re like me, you’ve had it drilled into your head that data should be presented to two decimal places. The only exception is p-values, where they can go to three decimal places if (and only if) p < .001. Today’s blog post is challenging that notion, and giving a new one: no more than two significant digits, please.

First, what’s a significant digit? Cole (2015) defines this as “the number of all digits ignoring the decimal point, and ignoring all leading zeros and some trailing zeros” (p. 608). Van Belle (2011) and Ehrenberg (1977) have similar rules, although they use the term effective digits instead of significant digits.

Okay, what’s a leading zero and a trailing zero? A good example of a leading zero comes from correlations. If you report *r* = 0.50, the 0 before the decimal point is the leading zero. Good examples of trailing zeros come from presenting results from population-level datasets. If you report *n* = 325,000 persons a part of a given category, the three 0s after the comma are trailing zeros.

Now that we have everything defined, let’s think this “two significant digits” rule through with an example. In a project I’m currently working on, I found the prevalence of binge drinking to be 14.06 per 100 persons. My training throughout undergraduate and graduate school suggests that is exactly how I should report the results in text – 14.06 per 100.

Let’s think about this though. What does 14.06 tell us that 14 doesn’t? I’d argue, very little and that I should just go with 14 per 100 instead. 

First, 14 is the point of two significant digits. My interpretation of the results is not going to change as the X in 14.X or 14.XX varies. 

Second, 14 per 100 is just easier to understand than 14.06. I can easily refer to this as 14%, both when reporting the results in-text, when presenting the results at a conference, and when discussing this finding with other researchers.

Third, it is going to make reporting my findings in tables easier. Since this is a descriptive epidemiology project, I’m going to be reporting both the overall prevalence and looking at prevalence among subgroups. Let’s take a look at the table below illustrating this.

**Table 1: Examples of Displaying Prevalence**

| __Variable__  | __Prevalence (per 100.00)__  | __Prevalence (per 100)__ | 
| ------------- | ---------------------------- | ------------------------ |
| Overall | 14.06 | 14 |
|*Age Group*  |  |  |
|18-24 | 17.22 | 17 |
|25-34 | 21.94 | 22 |
|35-44 | 36.78 | 37 |
|45-54 | 14.87 | 15 |
|55-64 | 11.89 | 12 |
|65+   | 5.24  | 5  | 

The prevalence per 100 column, without the two additional decimal places, is much cleaner and easier to read while also communicating the same information. The additional information provided by the two extra decimal places does not make the results more precise – really, it just makes the results more confusing.

This two significant digit principle can be applied to nearly all instances of statistical reporting. Consider a population health study where you have sample sizes of 1,515,235, 1,621,351, and 1,444,346. What do these numbers tell you that 1.5 million, 1.6 million, and 1.4 million do not? What does a χ² value of 237.98 tell you that a value of 238 does not? What does *r* = 0.78 tell you that *r* = .78 does not? In all these cases, reporting out to the two significant digits is much easier to comprehend without changing the precision of results.

While two significant digits is a good guideline, it is still on you as the researcher to determine exactly what the two significant digits are, and if you need to use one significant digit or more than two significant digits. Consider p-values greater than .10. Do we need to know *p* = .23? Not really, so *p* = .2 is enough. On the other hand, if you are reporting percent change over time, you may need to report higher than 100%, which would be three significant digits. Cole (2015) also advocates that in some cases you may want to follow the rule of four significant digits when reporting risk or odds ratios, especially if the values are very close to 1.

Ultimately though, this post can be summed up as another extension of the parsimony principle. We want things to be as simple as possible, but no simpler. This applies with the reporting of results just as much as it applies to model building. So, when it comes to presenting your data, it’s time to let go of the “two decimal place” rule and adopt the “two significant digits” rule instead. It’ll make things simpler, more comprehensible, and no less precise.

Unless of course, an editor makes you go out to the two decimal places. Then, you gotta do what you gotta do to get published.

## References

Cole, T. J. (2015). Too many digits: the presentation of numerical data. Archives of Disease in Childhood, 100(7), 608-609.

Ehrenberg, A. S. (1977). Rudiments of numeracy. Journal of the Royal Statistical Society: Series A (General), 140(3), 277-297.

Van Belle, G. (2011). Statistical rules of thumb. John Wiley & Sons.

# On Reflective and Formative Items of Latent Variables

## **A**dvanced **QUA**n**TI**tative topics to **C**heckout (AQUATIC) Blog

### Zach Kunicki, QSP Postdoc

Hey AQUATIC readers, today post is about psychological measurement theory. Specifically, we’re going to review the differences between *reflective* and *formative* latent variables.

First, let’s review what we mean by *latent variables*. A latent variable is any concept we think can be used to explain something, but that we cannot directly measure. Let’s consider two examples of what we mean by this definition.

Depression is a commonly measured latent variable. Research suggests depression can, in part, explain behaviors (i.e., disrupted sleep) and mood (i.e., anhedonia, low energy). However, we cannot just look at someone and think “you are X% depressed” in the same way we can look at someone and see their hair style or clothing. We have to measure depression <u>indirectly</u>. A common way of perfomring indirect measurement is through administering some type of insrument, like the Beck Depression Inventory (BDI) or Depression Anxiety Stress Scales (DASS-21).

Socioeconomic status (SES) is another commonly measured latent variable. SES, in part, can explain behaviors (i.e., substance use, choice of leisure activity), mood (i.e., stress), and health outcomes (i.e., likelihood of developing and the severity of both mental and physical illnesses). Again though, we cannot just look at someone and say “You are in X SES class.” Instead, we ask individuals about their (and/or their family’s) income, education, and occupation to determine their level of SES. This is another form of indirect measurement.

For many researchers, there is a fundamental conceptual distinction between how we measure traits like depression and SES. The distionction is known by the labels *reflective* and *formative* measurement. In a *reflective latent variable model, we assume that* the **latent variable causes the response to the items.** In a *formative latent variable model, we assume that* **the items cause the latent variable** (Bollen, 2002; Borsboom et al., 2003; Jones et al., 2011).

This difference in causality has huge implications for psychological measurement. Perhaps the biggest one is that a reflective latent variable, we assume, exists *before we measure it*. A formative latent variable, on the other hand, exists *only when we measure it*.

When we measure depression, we assume that the individual has some amount of depression, and that amount of depression will cause them to answer psychological measures in a certain way. Consider the item “I felt that I had nothing to look forward to,” which is part of the DASS-21 depression subscale (Antony et al., 1998).  The response options are “0 = did not apply to me at all” to “3 = Applied to me very much or most of the time.” If we assume depression is a reflective latent variable, we assume that people who are MORE depressed will have higher scores on this, and the other 6 depression items, than people who are LESS depressed. But, regardless of the individual’s actual response, *their level of depression before they took the measure causes their answers*. This is what we mean by a reflective latent variable – it reflects the individual.

If we had a treatment for depression in general, we should expect that this treatment would reduce all symptom reporting including “I felt I had nothing to look forward to”. If we had a treatment specific for hopelessness, we would expect that this treatment would **not** influence other aspects of depression (e.g., anhedonia, dysphoria, sleep disturbance).

When we measure SES, we assume people with higher incomes, high levels of education, and more occupational prestige are in a higher SES category than persons with lower incomes, lower levels of education, and less occupational prestige. Again – it does not matter what an individual’s actual income, education, and occupation are, what matter is *these three variables cause the level of the latent variable*.  This is what we mean by a formative latent variable – it is formed by the items that make it up.

If we had a treatment for low income (e.g., minimum basic income) we expect that this would elevate “overall SES” but would not influence education or occupation and any other formative indicator of SES. However, as it is currently conceptualized, we do have a global treatment for SES.

Because the direction of causality is different for reflective and formative latent variables, we should take this into account when we perform psychological measurement. When assessing reflective latent variables, we need to use approaches such as factor analysis and item response theory, as those measurement techniques assume the latent variable influences the individual items. However, since formative measurement assume the items influence the latent variable, we should use approaches like multiple regression or principal components analysis  which assume the items influence the latent variable – the exact same way a predictor influences an outcome. 

Now, how does one determine if they are working with a formative or a reflective variable? First and foremost, it should be driven by theory. If there is no articulated theory, then the analyst will imply one through the choice of a reflective or formative model. However, applying a reflective measurement model to the indicators of SES does not make SES a reflective construct. It means the analyst made a mistake, but it is an easy mistake to make. The best we can do that can be done is to conduct (or capitalize on natural) experiments like the ones mentioned above (treatments for depression, treatment for low income) and see how these treatments flow to other indicators in the system.

For more reading in this area, see Bollen (2002), Borsboom et al. (2003), and Borsboom (2005) for the mathematical and philosophical differences between these two types latent variables. See Jones et al. (2011) for an applied example illustrating these differences in the context of cognitive reserve. 

## References

Antony, M. M., Bieling, P. J., Cox, B. J., Enns, M. W., & Swinson, R. P. (1998). Psychometric properties of the 42-item and 21-item versions of the Depression Anxiety Stress Scales in clinical groups and a community sample. Psychological Assessment, 10(2), 176-181. 

Bollen, K. A. (2002). Latent Variables in Psychology and the Social Sciences. Annual review of psychology, 53(1), 605-634. https://doi.org/10.1146/annurev.psych.53.100901.135239 

Borsboom, D. (2005). Measuring the mind: Conceptual issues in contemporary psychometrics. Cambridge University Press. 

Borsboom, D., Mellenbergh, G. J., & van Heerden, J. (2003). The theoretical status of latent variables. Psychological Review, 110(2), 203-219. https://doi.org/10.1037/0033-295X.110.2.203 

Jones, R. N., Manly, J., Glymour, M. M., Rentz, D. M., Jefferson, A. L., & Stern, Y. (2011). Conceptual and measurement challenges in research on cognitive reserve. Journal of the International Neuropsychological Society : JINS, 17(4), 593-601. https://doi.org/10.1017/S1355617710001748 